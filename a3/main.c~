// Same socket 16 procs: mpirun -mca plm_rsh_no_tree_spawn 1 --map-by ppr:16:socket -bind-to core -np 16 ./p1
// Between sockets 16 procs: mpirun -mca plm_rsh_no_tree_spawn 1 --map-by socket -bind-to core -np 16 ./p1
// Between nodes 16 procs: mpirun -mca plm_rsh_no_tree_spawn 1 --map-by node -bind-to core -np 16 -host en-openmpi00,en-openmpi02 ./p1
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <stdbool.h>
#include <mpi.h>
#include <time.h>
#include <unistd.h>


#define NUM_RUNS (1<<10)
//max msg length in bytes
#define MAX_MSG_LEN (1<<20)
#define DEBUG(S, ...) printf("DEBUG (rank %d of %d): " S "\n", myrank, nproc, ##__VA_ARGS__)
// Only the rank 0 process debugs
#define DEBUGR0(S, ...) if(ROOT()) DEBUG(S, ##__VA_ARGS__)

// The root node has rank 0
#define ROOT() !myrank

inline void ping(int p, char *buf, size_t size);
inline void pong(int p, char *buf, size_t size);


typedef enum mpi_tag
{
    TAG_MSG = 0,
    TAG_RUNTIME,
} mpi_tag_t;

static int myrank, nproc;

int main(int argc, char **argv)
{
    int ierr;

    ierr = MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
    MPI_Comm_size(MPI_COMM_WORLD, &nproc);

    //allocate the max size buffer, so we only need to do it once
    char *buf = malloc(MAX_MSG_LEN);

    if (!buf) {
	DEBUG("Malloc failed!!");
	exit(1);
    }

    int name_len;
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    MPI_Get_processor_name(processor_name, &name_len);
    //printf("Hello, my names is %s\n", processor_name);

    FILE *fd;
    if (ROOT()) {
	char name[20];
	snprintf(name, sizeof(name), "%d_procs_comm.csv", nproc);
	fd = fopen(name, "w");
	fprintf(fd, "Length,ns/byte\n");
	DEBUGR0("Ping-Pong: %d processes, 1 - %u bytes", nproc, MAX_MSG_LEN);
	
    }



    for (uint64_t len=1; len <= MAX_MSG_LEN; len <<= 1)
    {
	double start=MPI_Wtime(); /*start timer*/

	// transfer data
	for (uint32_t n=0; n < NUM_RUNS; n++) {
	    // Am I a pinger or ponger first?
	    if (!(myrank%2)) {
		//Pinger ranks: 0, 2, 4,...
		ping((myrank+1)%nproc, buf, len);
		pong((myrank+1)%nproc, buf, len);
	    } else { 
		//Ponger ranks: 1, 3, 5...
		pong((myrank+nproc-1)%nproc, buf, len);
		ping((myrank+nproc-1)%nproc, buf, len);
	    }
	}

	double end = MPI_Wtime();
    
	double avg_runtime = (end - start)/(NUM_RUNS);

	//DEBUG("Runtime: %f", end-start);

	//get data
	double res;
	MPI_Reduce(&avg_runtime, &res, 1,
		       MPI_DOUBLE, MPI_SUM, 0,
		       MPI_COMM_WORLD);
	
	//get data
	//MPI_Gather(&avg_runtime, 1, MPI_DOUBLE, res, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);

	
	if (!myrank) {
	    // average it out
	    res /= nproc;
	    double norm_runtime = (1000000000*res)/len;
	    DEBUG("Got all process results for %lu bytes: %f ns/byte", len, norm_runtime);
	    fprintf(fd, "%lu,%f\n", len, norm_runtime);
	}
	

    }
    //Done!
    if (ROOT())
	fclose(fd);
    ierr = MPI_Finalize();
}


void ping(int p, char *buf, size_t size)
{
    MPI_Send(buf, size, MPI_CHAR, p, TAG_MSG, MPI_COMM_WORLD);
        
}


void pong(int p, char *buf, size_t size)
{
    MPI_Recv(buf, size, MPI_CHAR, p, TAG_MSG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}
